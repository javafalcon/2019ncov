{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, initializers, models, optimizers, callbacks\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(vectors, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_vector = dim_vector\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(CapsuleLayer, self).build(input_shape)\n",
    "\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_vector = input_shape[2]\n",
    "        \n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n",
    "                                initializer=self.kernel_initializer,\n",
    "                                name='W')\n",
    "        \n",
    "        self.bias = self.add_weight(shape=[1,self.input_num_capsule,self.num_capsule,1,1],\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   name='bias',\n",
    "                                   trainable=False)\n",
    "        self.built = True\n",
    "                \n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None,input_num_capsule,input_dim_vector]\n",
    "        # Expand dims to [None,input_num_capsule,1,1,input_dim_vector]\n",
    "        inputs_expand = K.expand_dims(K.expand_dims(inputs,2),2)\n",
    "        \n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # Now it has shape = [None,input_num_capsule,num_capsule,1,input_dim_vector]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1,1,self.num_capsule,1,1])\n",
    "        \n",
    "        # Begin: inputs_hat computation V2\n",
    "        # Compute 'inputs * W' by scanning inputs_tiled on dimension 0. \n",
    "        # inputs_hat.shape = [None, input_num_capsule,num_capsule,1,dim_vector]\n",
    "        inp = K.reshape(inputs_tiled,(-1, self.input_num_capsule*self.num_capsule,1,self.input_dim_vector))\n",
    "        w = K.reshape(self.W, (self.input_num_capsule*self.num_capsule, self.input_dim_vector,self.dim_vector))\n",
    "        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, w, [2,1]),\n",
    "                            elems=inp,\n",
    "                            initializer=K.zeros([self.input_num_capsule*self.num_capsule,1,self.dim_vector]))\n",
    "        inputs_hat = K.reshape(inputs_hat, (-1, self.input_num_capsule, self.num_capsule, 1, self.dim_vector))\n",
    "        \n",
    "        # Begin: routing algorithm V2\n",
    "        # Routing alogrithm V2. Use iteration. V2 and V1 both work without much difference on performace\n",
    "        assert self.num_routing > 0, 'The num_routing should be > 0'\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(self.bias, axis=2)\n",
    "            outputs = squash(K.sum(c*inputs_hat, 1, keepdims=True))\n",
    "            \n",
    "            # last iteration needs not compute the bias which will not be passed to the graph any more anyway.\n",
    "            if i != self.num_routing - 1:\n",
    "                (self.bias).assign_add(K.sum(inputs_hat*outputs, -1, keepdims=True))\n",
    "                \n",
    "        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Length(layers.Layer):    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mask(layers.Layer):   \n",
    "    def call(self, inputs, **kwargs):\n",
    "        # use true label to select target capsule, shape=[batch_size, num_capsule]\n",
    "        if type(inputs) is list: # true label is provided with shape=[batch_size, n_classes]\n",
    "            assert len(inputs)==2\n",
    "            inputs, mask = inputs\n",
    "        else: # if no true label, mask by the max length of vectors of capsule\n",
    "            x = inputs\n",
    "            x = (x - K.max(x, 1, True)) / K.epsilon() + 1\n",
    "            mask = K.clip(x, 0, 1)\n",
    "        \n",
    "        # masked inputs, shape=[batch_size, dim_vector]\n",
    "        inputs_masked = K.batch_dot(inputs, mask, [1,1])\n",
    "        return inputs_masked\n",
    "    \n",
    "    def computer_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tupel: # true lable provided\n",
    "            return tuple([None, input_shape[0][-1]])\n",
    "        else:\n",
    "            return tuple([None, input_shape[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n",
    "    output = layers.Conv2D(filters=dim_vector*n_channels, kernel_size=kernel_size,\n",
    "                          strides=strides, padding=padding,\n",
    "                          name='primaryCap_conv2d')(inputs)\n",
    "    dim = output.shape[1]*output.shape[2]*output.shape[3]\n",
    "    outputs = layers.Reshape(target_shape=(dim//dim_vector,dim_vector), name='primaryCap_reshape')(output)\n",
    "    #outputs = layers.Reshape(target_shape=[-1,dim_vector], name='primaryCap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "def CapsNet(input_shape, n_class, num_routing):\n",
    "    x = layers.Input(shape=input_shape)\n",
    "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid',\n",
    "                         activation='relu', name='conv1')(x)\n",
    "    primarycaps = PrimaryCap(conv1, dim_vector=8, n_channels=32, kernel_size=9, \n",
    "                            strides=2, padding='valid')\n",
    "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_vector=16, num_routing=num_routing,\n",
    "                            name='digitcaps')(primarycaps)\n",
    "    out_caps = Length(name='out_caps')(digitcaps)\n",
    "    \n",
    "    # Decoder network\n",
    "    y = layers.Input(shape=(n_class,))\n",
    "    masked = Mask()([digitcaps, y])\n",
    "    x_recon = layers.Dense(512, activation='relu')(masked)\n",
    "    x_recon = layers.Dense(1024, activation='relu')(x_recon)\n",
    "    x_recon = layers.Dense(np.prod(input_shape), activation='sigmoid')(x_recon)\n",
    "    x_recon = layers.Reshape(target_shape=input_shape, name='out_recon')(x_recon)\n",
    "    \n",
    "    return models.Model([x,y], [out_caps, x_recon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(y_true, y_pred):\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "    return K.mean(K.sum(L,1))\n",
    "\n",
    "def train(model, data, lr=0.001, lam_recon=0.39, \n",
    "          batch_size=100, epochs=10):\n",
    "    (x_train, y_train),(x_test, y_test) = data\n",
    "    \n",
    "    model.compile(optimizer=optimizers.Adam(lr=lr),\n",
    "                 loss=[margin_loss, 'mse'],\n",
    "                 loss_weights=[1., lam_recon],\n",
    "                 metrics={'out_caps': 'accuracy'})\n",
    "    model.fit([x_train, y_train], [y_train, x_train],\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=[[x_test,y_test],[y_test,x_test]])\n",
    "    \n",
    "def load_mnist():\n",
    "    from tensorflow.keras.datasets import mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(-1,28,28,1).astype('float32') / 255.\n",
    "    x_test = x_test.reshape(-1,28,28,1).astype('float32') / 255.\n",
    "    y_train = to_categorical(y_train.astype('float32'))\n",
    "    y_test = to_categorical(y_test.astype('float32'))\n",
    "    \n",
    "    return (x_train, y_train),(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = load_mnist()\n",
    "model = CapsNet(input_shape=[28,28,1], n_class=10, num_routing=3)\n",
    "model.summary()\n",
    "train(model=model,data=((x_train,y_train),(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu] *",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
